{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5. 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.1. 加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储张量列表并读取\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储张量字典并读取\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.2. 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.output(F.relu(self.hidden(X)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn((2, 20))\n",
    "Y = net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建类对象并加载参数\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.3. 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save和load函数可用于张量对象的文件读写。\n",
    "\n",
    "我们可以通过参数字典保存和加载网络的全部参数。\n",
    "\n",
    "保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.4. 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？\n",
    "\n",
    "torch.load('mlp.params')加载的是一个mlp参数，只要改为'hidden.params'就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "传入参数字典为：\n",
      "OrderedDict([('hidden.weight', tensor([[ 0.1864,  0.0333,  0.1993,  ..., -0.0204, -0.0569,  0.1486],\n",
      "        [-0.2091, -0.0233, -0.0200,  ...,  0.2099, -0.1445, -0.0497],\n",
      "        [-0.1450, -0.0436, -0.1642,  ..., -0.0716,  0.2026, -0.0758],\n",
      "        ...,\n",
      "        [ 0.1809,  0.0228,  0.1010,  ..., -0.1003,  0.1658,  0.0707],\n",
      "        [-0.1288,  0.1635,  0.0067,  ..., -0.2007, -0.0681,  0.1416],\n",
      "        [ 0.1655, -0.0204, -0.1610,  ...,  0.0170,  0.0091, -0.0609]])), ('hidden.bias', tensor([ 0.0237, -0.0862,  0.0789,  0.0789, -0.1755,  0.2051,  0.1058,  0.0100,\n",
      "         0.1728, -0.0918, -0.0369, -0.0316, -0.0409, -0.1581,  0.1483,  0.0857,\n",
      "         0.0377, -0.1134, -0.1761,  0.0600,  0.1407, -0.0715, -0.0274, -0.1015,\n",
      "        -0.1523, -0.0132,  0.1089, -0.1958,  0.1569, -0.0553,  0.0255, -0.1272,\n",
      "         0.1278,  0.1761,  0.2218, -0.1923, -0.0600,  0.1455, -0.1990,  0.1279,\n",
      "        -0.0128,  0.0869,  0.1181,  0.1380,  0.0523, -0.1921, -0.2061, -0.1560,\n",
      "        -0.1760, -0.2194,  0.0761,  0.0832, -0.0906,  0.2111,  0.0618, -0.1694,\n",
      "         0.0332, -0.1056, -0.0750,  0.0946,  0.0122, -0.0111,  0.2012, -0.2024,\n",
      "        -0.1906, -0.2231, -0.1888, -0.1468,  0.1587, -0.0930,  0.0048, -0.0251,\n",
      "         0.0656,  0.1679, -0.0849, -0.2065, -0.1428,  0.0868, -0.0643,  0.1862,\n",
      "         0.1095,  0.1655, -0.0021,  0.0328, -0.0741, -0.0678,  0.1316,  0.1702,\n",
      "        -0.0986,  0.1944, -0.0181,  0.1264, -0.0459, -0.0206, -0.1010, -0.0675,\n",
      "         0.0935,  0.1857, -0.0225, -0.2099,  0.0559, -0.0193,  0.0724,  0.2214,\n",
      "         0.1424,  0.0301,  0.1215, -0.1309, -0.0243, -0.1810, -0.1930, -0.0388,\n",
      "         0.0296, -0.0820, -0.0112,  0.1764,  0.0944,  0.0666,  0.1032, -0.0323,\n",
      "        -0.0673, -0.0430, -0.1311,  0.1829,  0.0961,  0.1526, -0.1369, -0.1009,\n",
      "        -0.0974, -0.1231,  0.0154, -0.1824, -0.1688,  0.0969,  0.1930, -0.1540,\n",
      "        -0.2161, -0.1286,  0.1328,  0.0687, -0.1380,  0.1222, -0.0474,  0.0974,\n",
      "        -0.0945, -0.1898,  0.1446,  0.1790,  0.1850, -0.1369, -0.0472, -0.2091,\n",
      "        -0.1511, -0.0682, -0.0929,  0.1465,  0.0831, -0.2018,  0.0268,  0.2004,\n",
      "         0.0220, -0.0485,  0.1130,  0.1760,  0.0529, -0.0838, -0.0668,  0.1373,\n",
      "         0.1223, -0.1930,  0.0824, -0.0139,  0.2162, -0.1927,  0.0273, -0.0638,\n",
      "         0.0082, -0.1517,  0.1034, -0.1094, -0.0537,  0.1430,  0.0054, -0.1760,\n",
      "         0.0376, -0.1485, -0.1246, -0.0146,  0.2105, -0.0182, -0.1909,  0.1914,\n",
      "         0.1712,  0.1934, -0.1713,  0.0064,  0.2054,  0.1074, -0.1599,  0.2133,\n",
      "        -0.2224,  0.1265,  0.1858, -0.1297,  0.0994, -0.2016, -0.1501, -0.0025,\n",
      "        -0.0228, -0.0714,  0.0589,  0.1287,  0.0232, -0.0277, -0.1795, -0.1473,\n",
      "        -0.1611, -0.1264,  0.1185,  0.1085, -0.0664,  0.0426, -0.0599,  0.2042,\n",
      "        -0.1369, -0.2034,  0.0389, -0.1937,  0.0459,  0.1921, -0.1287,  0.0348,\n",
      "         0.0474,  0.0264,  0.0162,  0.0509, -0.0466, -0.2111,  0.1543, -0.1377,\n",
      "         0.0248, -0.0785,  0.1376,  0.1135,  0.1026, -0.1409,  0.0168,  0.2174,\n",
      "         0.0771,  0.0561,  0.1182, -0.0853, -0.0013,  0.1695,  0.2199, -0.0424])), ('output.weight', tensor([[-0.0328,  0.0020, -0.0040,  ...,  0.0620, -0.0619,  0.0620],\n",
      "        [-0.0614,  0.0132,  0.0360,  ..., -0.0086, -0.0485, -0.0006],\n",
      "        [ 0.0139,  0.0246,  0.0494,  ..., -0.0141,  0.0221, -0.0274],\n",
      "        ...,\n",
      "        [-0.0457, -0.0563, -0.0623,  ..., -0.0313, -0.0416,  0.0271],\n",
      "        [ 0.0284,  0.0010, -0.0497,  ...,  0.0061,  0.0030,  0.0086],\n",
      "        [-0.0555, -0.0337, -0.0050,  ..., -0.0587, -0.0144, -0.0360]])), ('output.bias', tensor([ 0.0227, -0.0556, -0.0170, -0.0353, -0.0084, -0.0029,  0.0076, -0.0488,\n",
      "        -0.0361,  0.0256]))])\n",
      "传入第一层参数为：\n",
      "{'weight': tensor([[ 0.1864,  0.0333,  0.1993,  ..., -0.0204, -0.0569,  0.1486],\n",
      "        [-0.2091, -0.0233, -0.0200,  ...,  0.2099, -0.1445, -0.0497],\n",
      "        [-0.1450, -0.0436, -0.1642,  ..., -0.0716,  0.2026, -0.0758],\n",
      "        ...,\n",
      "        [ 0.1809,  0.0228,  0.1010,  ..., -0.1003,  0.1658,  0.0707],\n",
      "        [-0.1288,  0.1635,  0.0067,  ..., -0.2007, -0.0681,  0.1416],\n",
      "        [ 0.1655, -0.0204, -0.1610,  ...,  0.0170,  0.0091, -0.0609]]), 'bias': tensor([ 0.0237, -0.0862,  0.0789,  0.0789, -0.1755,  0.2051,  0.1058,  0.0100,\n",
      "         0.1728, -0.0918, -0.0369, -0.0316, -0.0409, -0.1581,  0.1483,  0.0857,\n",
      "         0.0377, -0.1134, -0.1761,  0.0600,  0.1407, -0.0715, -0.0274, -0.1015,\n",
      "        -0.1523, -0.0132,  0.1089, -0.1958,  0.1569, -0.0553,  0.0255, -0.1272,\n",
      "         0.1278,  0.1761,  0.2218, -0.1923, -0.0600,  0.1455, -0.1990,  0.1279,\n",
      "        -0.0128,  0.0869,  0.1181,  0.1380,  0.0523, -0.1921, -0.2061, -0.1560,\n",
      "        -0.1760, -0.2194,  0.0761,  0.0832, -0.0906,  0.2111,  0.0618, -0.1694,\n",
      "         0.0332, -0.1056, -0.0750,  0.0946,  0.0122, -0.0111,  0.2012, -0.2024,\n",
      "        -0.1906, -0.2231, -0.1888, -0.1468,  0.1587, -0.0930,  0.0048, -0.0251,\n",
      "         0.0656,  0.1679, -0.0849, -0.2065, -0.1428,  0.0868, -0.0643,  0.1862,\n",
      "         0.1095,  0.1655, -0.0021,  0.0328, -0.0741, -0.0678,  0.1316,  0.1702,\n",
      "        -0.0986,  0.1944, -0.0181,  0.1264, -0.0459, -0.0206, -0.1010, -0.0675,\n",
      "         0.0935,  0.1857, -0.0225, -0.2099,  0.0559, -0.0193,  0.0724,  0.2214,\n",
      "         0.1424,  0.0301,  0.1215, -0.1309, -0.0243, -0.1810, -0.1930, -0.0388,\n",
      "         0.0296, -0.0820, -0.0112,  0.1764,  0.0944,  0.0666,  0.1032, -0.0323,\n",
      "        -0.0673, -0.0430, -0.1311,  0.1829,  0.0961,  0.1526, -0.1369, -0.1009,\n",
      "        -0.0974, -0.1231,  0.0154, -0.1824, -0.1688,  0.0969,  0.1930, -0.1540,\n",
      "        -0.2161, -0.1286,  0.1328,  0.0687, -0.1380,  0.1222, -0.0474,  0.0974,\n",
      "        -0.0945, -0.1898,  0.1446,  0.1790,  0.1850, -0.1369, -0.0472, -0.2091,\n",
      "        -0.1511, -0.0682, -0.0929,  0.1465,  0.0831, -0.2018,  0.0268,  0.2004,\n",
      "         0.0220, -0.0485,  0.1130,  0.1760,  0.0529, -0.0838, -0.0668,  0.1373,\n",
      "         0.1223, -0.1930,  0.0824, -0.0139,  0.2162, -0.1927,  0.0273, -0.0638,\n",
      "         0.0082, -0.1517,  0.1034, -0.1094, -0.0537,  0.1430,  0.0054, -0.1760,\n",
      "         0.0376, -0.1485, -0.1246, -0.0146,  0.2105, -0.0182, -0.1909,  0.1914,\n",
      "         0.1712,  0.1934, -0.1713,  0.0064,  0.2054,  0.1074, -0.1599,  0.2133,\n",
      "        -0.2224,  0.1265,  0.1858, -0.1297,  0.0994, -0.2016, -0.1501, -0.0025,\n",
      "        -0.0228, -0.0714,  0.0589,  0.1287,  0.0232, -0.0277, -0.1795, -0.1473,\n",
      "        -0.1611, -0.1264,  0.1185,  0.1085, -0.0664,  0.0426, -0.0599,  0.2042,\n",
      "        -0.1369, -0.2034,  0.0389, -0.1937,  0.0459,  0.1921, -0.1287,  0.0348,\n",
      "         0.0474,  0.0264,  0.0162,  0.0509, -0.0466, -0.2111,  0.1543, -0.1377,\n",
      "         0.0248, -0.0785,  0.1376,  0.1135,  0.1026, -0.1409,  0.0168,  0.2174,\n",
      "         0.0771,  0.0561,  0.1182, -0.0853, -0.0013,  0.1695,  0.2199, -0.0424])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP1(\n",
       "  (output): Linear(in_features=20, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = nn.Linear(20, 256)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.output(X)\n",
    "\n",
    "clone_part = MLP1()\n",
    "mlp_dict = torch.load('mlp.params')\n",
    "\n",
    "temp = {'weight':net.state_dict()['hidden.weight'],'bias':net.state_dict()['hidden.bias']}\n",
    "torch.save(temp,'hidden.params')\n",
    "mlp_hidden_dict = torch.load('hidden.params')\n",
    "print(\"传入参数字典为：\")\n",
    "print(mlp_dict)\n",
    "print(\"传入第一层参数为：\")\n",
    "print(mlp_hidden_dict)\n",
    "\n",
    "clone_part.load_state_dict(torch.load('hidden.params'),strict=False)\n",
    "clone_part.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
